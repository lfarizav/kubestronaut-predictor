# .github/workflows/mlops-pipeline.yml
name: MLOps FastAI Pipeline
on:
 workflow_dispatch:
   inputs:
     run-all:
       description: 'Run all jobs'
       required: false
       default: 'true'
     run_data_processing:
       description: 'Run data processing job'
       required: false
       default: 'false'
     run_model_training:
       description: 'Run model training job'
       required: false
       default: 'false'
     run_build_and_publish:
       description: 'Run build and publish job'
       required: false
       default: 'false'
jobs:
 data-processing:
   runs-on: ubuntu-latest

   steps:
   - name: Checkout code
     uses: actions/checkout@v2

   - name: Set up Python
     uses: actions/setup-python@v2
     with:
       python-version: '3.11.13'

   - name: Install dependencies
     run: |
       python -m pip install --upgrade pip
       pip install -r requirements.txt

   - name: Process data
     run: |
       python src/data/run_processing.py \
       --input data/raw/kubestronaut_predictor_data.csv \
       --output data/processed/cleaned_kubestronaut_predictor_data.csv

   - name: Engineer features
     run: |
       python src/features/engineer.py \
       --input data/processed/cleaned_kubestronaut_predictor_data.csv \
       --output data/processed/featured_kubestronaut_predictor_data.csv \
       --preprocessor models/trained/preprocessor.pkl

   - name: Upload processed data
     uses: actions/upload-artifact@v4
     with:
       name: processed-data
       path: data/processed/featured_kubestronaut_predictor_data.csv

   - name: Upload preprocessor
     uses: actions/upload-artifact@v4
     with:
       name: preprocessor
       path: models/trained/preprocessor.pkl

 model-training:
   needs: data-processing
   runs-on: ubuntu-latest

   steps:
   - name: Checkout code
     uses: actions/checkout@v2

   - name: Set up Python
     uses: actions/setup-python@v2
     with:
       python-version: '3.11.13'

   - name: Install dependencies
     run: |
       python -m pip install --upgrade pip
       pip install -r requirements.txt

   - name: Download processed data
     uses: actions/download-artifact@v4
     with:
       name: processed-data
       path: data/processed/

   - name: Set up MLflow
     run: |
       docker pull ghcr.io/mlflow/mlflow:latest
       docker run -d -p 5000:5000 \
       --name mlflow-server ghcr.io/mlflow/mlflow:latest mlflow server \
       --host 0.0.0.0 \
       --backend-store-uri sqlite:///mlflow.db

   - name: Wait for MLflow to start
     run: |
       for i in {1..10}; do
         curl -f http://localhost:5000/health || sleep 5;
       done

   - name: Train model
     run: |
       mkdir -p models
       python src/models/train_model.py --config configs/model_config.yaml \
       --data data/processed/featured_kubestronaut_predictor_data.csv \
       --models-dir models \
       --mlflow-tracking-uri http://localhost:5000

   - name: Upload trained model
     uses: actions/upload-artifact@v4
     with:
       name: trained-model
       path: models/

   - name: Clean up MLflow
     run: |
       docker stop mlflow-server || true
       docker rm mlflow-server || true

 build-and-publish-fastapi:
   needs: model-training
   runs-on: ubuntu-latest
   env:
      DOCKERHUB_USERNAME: ${{ vars.DOCKERHUB_USERNAME }}   # <-- from Variables
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}       # <-- from Secrets
      IMAGE_NAME: docker.io/${{ vars.DOCKERHUB_USERNAME }}/kubestronautfastapi

   steps:
   - name: Checkout code
     uses: actions/checkout@v2

   - name: Download trained model
     uses: actions/download-artifact@v4
     with:
       name: trained-model
       path: models/

   - name: Download preprocessor
     uses: actions/download-artifact@v4
     with:
       name: preprocessor
       path: models/trained/
   - name: Set up Docker Buildx
     uses: docker/setup-buildx-action@v3
   - name: Log in to DockerHub Container Registry
     uses: docker/login-action@v3
     with:
       registry: docker.io
       username: ${{ vars.DOCKERHUB_USERNAME }}
       password: ${{ secrets.DOCKERHUB_TOKEN }}

   - name: Build, test, and push Docker image
     run: |
       COMMIT_HASH=$(echo $GITHUB_SHA | cut -c1-7)
       BUILD_DATE=$(date -u +'%Y%m%d%H%M%S')
       IMAGE_TAG="${BUILD_DATE}-${COMMIT_HASH}"

       echo "Building image: ${IMAGE_NAME}:${IMAGE_TAG}"
       docker build -t ${IMAGE_NAME}:${IMAGE_TAG} -f Dockerfile .

       echo "Running container for smoke test..."
       docker run -d -p 8000:8000 --name test-api ${IMAGE_NAME}:${IMAGE_TAG}
       for i in {1..10}; do
         curl -sf http://localhost:8000/health && break || sleep 5;
       done
       docker logs test-api

       echo "Cleaning up test container"
       docker stop test-api || true
       docker rm test-api || true

       echo "Tagging image as latest"
       docker tag ${IMAGE_NAME}:${IMAGE_TAG} ${IMAGE_NAME}:latest

       echo "Pushing both tags"
       docker push ${IMAGE_NAME}:${IMAGE_TAG}
       docker push ${IMAGE_NAME}:latest

       echo "âœ… Pushed: ${IMAGE_NAME}:${IMAGE_TAG} and latest"