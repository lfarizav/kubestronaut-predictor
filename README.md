<table style="border-collapse: collapse; border: none;">
  <tr style="border-collapse: collapse; border: none;">
    <td style="border-collapse: collapse; border: none;">
      <a href="http://www.openairinterface.org/">
         <img src="https://gitlab.eurecom.fr/uploads/-/system/user/avatar/716/avatar.png?width=800" alt="" border=3 height=50 width=50>
         </img>
      </a>
    </td>
    <td style="border-collapse: collapse; border: none; vertical-align: center;">
      <b><font size = "5">üè† Kubestronaut Certification Predictor</font></b>
    </td>
  </tr>
</table>

# Author
**Luis Felipe Ariza Vesga** 
Emails: lfarizav@gmail.com, lfarizav@unal.edu.co
# Description
Welcome to the **Kubestronaut Certification Predictor** project! This is a fake modified project forked from https://github.com/mlopsbootcamp/hosue-price-predictor, end-to-end MLOps use case designed to help you master the art of building and operationalizing machine learning pipelines.

You'll start from raw data and move through data preprocessing, feature engineering, experimentation, model tracking with MLflow, and optionally using Jupyter for exploration ‚Äì all while applying industry-grade tooling.

---

## üì¶ Project Structure

```
kuestronaut-certification-predictor/
‚îú‚îÄ‚îÄ configs/                # YAML-based configuration for models
‚îú‚îÄ‚îÄ data/                   # Raw and processed datasets
‚îú‚îÄ‚îÄ deployment/
‚îÇ   ‚îî‚îÄ‚îÄ mlflow/             # Docker Compose setup for MLflow
‚îú‚îÄ‚îÄ models/                 # Trained models and preprocessors
‚îú‚îÄ‚îÄ notebooks/              # Optional Jupyter notebooks for experimentation
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/               # Data cleaning and preprocessing scripts
‚îÇ   ‚îú‚îÄ‚îÄ features/           # Feature engineering pipeline
‚îÇ   ‚îú‚îÄ‚îÄ models/             # Model training and evaluation
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îî‚îÄ‚îÄ README.md               # You‚Äôre here!
```

---

## üõ†Ô∏è Setting up Learning/Development Environment

To begin, ensure the following tools are installed on your system:

- [Python 3.11](https://www.python.org/downloads/)
- [Git](https://git-scm.com/)
- [Visual Studio Code](https://code.visualstudio.com/) or your preferred editor
- [UV ‚Äì Python package and environment manager](https://github.com/astral-sh/uv)
- [Docker Desktop](https://www.docker.com/products/docker-desktop/) **or** [Podman Desktop](https://podman-desktop.io/)

---

## üöÄ Preparing Your Environment

1. **Fork this repo** on GitHub.

2. **Clone your forked copy:**

   ```bash
   git clone https://github.com/lfarizav/kubestronaut-predictor.git
   cd kubestronaut-predictor
   ```

3. **Setup Python Virtual Environment using UV:**

   ```bash
   uv venv --python python3.11
   source .venv/bin/activate
   ```

4. **Install dependencies:**

   ```bash
   uv pip install -r requirements.txt
   ```

---

## üìä Setup MLflow for Experiment Tracking

To track experiments and model runs:

```bash
cd deployment/mlflow
docker compose up -d
docker compose ps
```

> üêß **Using Podman?** Use this instead:

```bash
cd deployment/mlflow
podman compose up -d
podman compose ps
```

Access the MLflow UI at [http://localhost:5555](http://localhost:5555)

---

## üìí Using JupyterLab (Optional)

If you prefer an interactive experience, launch JupyterLab with:

```bash
uv python -m jupyterlab
# or
python -m jupyterlab
```

---

## üîÅ Model Workflow

### üßπ Step 1: Data Processing

Clean and preprocess the raw housing dataset:

```bash
python src/data/run_processing.py   --input data/raw/kubestronaut_predictor_data.csv   --output data/processed/cleaned_kubestronaut_predictor_data.csv
```

---

### üß† Step 2: Feature Engineering

Apply transformations and generate features:

```bash
python src/features/engineer.py   --input data/processed/cleaned_kubestronaut_predictor_data.csv   --output data/processed/featured_kubestronaut_predictor_data.csv   --preprocessor models/trained/preprocessor.pkl
```

---

### üìà Step 3: Modeling & Experimentation

Train your model and log everything to MLflow:

```bash
python src/models/train_model.py   --config configs/model_config.yaml   --data data/processed/featured_kubestronaut_predictor_data.csv   --models-dir models   --mlflow-tracking-uri http://localhost:5555
```

---


## Building FastAPI and Streamlit 

The code for both the apps are available in `src/api` and `streamlit_app` already. To build and launch these apps 

  * Add a  `Dockerfile` in the root of the source code for building FastAPI  
  * Add `streamlit_app/Dockerfile` to package and build the Streamlit app  
  * Add `docker-compose.yaml` in the root path to launch both these apps. be sure to provide `API_URL=http://fastapi:8000` in the streamlit app's environment. 


Once you have launched both the apps, you should be able to access streamlit web ui and make predictions. 

You could also test predictions with FastAPI directly using 

```
curl -X POST "http://localhost:8000/predict" \
-H "Content-Type: application/json" \
-d '{
  "theory_hours": 1500,
  "lab_hours": 1500,
  "number_full_exam_done": 2,
  "cncf_try_numbers": 2,
  "location": "suburban",
  "born_year": 2000,
  "selfassessment": "poor"
}'

```

Be sure to replace `http://localhost:8000/predict` with actual endpoint based on where its running. 

## Create Continuous Integration (CI) using GitHub Actions

### Create the folder .github/workflows
Inside the main folder, create a hidden folder

```bash
cd ~/kubestronaut-predictor
mkdir .github
cd .github
mkdir workflows
cd workflows
```
Inside the .github/workflows folder we create the *.yaml files with the pipeline that GitHub execute once a push o merge request is performed.

## Create Continuous Deployment (CD) using Flux
We follow good practices, in this project we organiza code and  infrastructure in different GitHub repositories. In total we have 3 repositories: kubestronaut-predictor with the code, kubestronaut-predictor-deploy with the infrastructure code, and kubestronaut-predictor-fleet with the information of flux when a bootstrap is executed.

```bash
git clone https://github.com/lfarizav/kubestronaut-predictor-fleet.git

```

## üß† Learn More About MLOps

In this project you'll learn how to:

- Build and track ML pipelines
- Containerize and deploy models
- Automate training workflows using GitHub Actions or Argo or Flux or Jenkins Workflows
- Apply DevOps principles to Machine Learning systems

---

Happy Learning!  
‚Äî Team **De la parla al cluster**
üîó [More info at ‚Üí](https://delaparlaalcluster.org)
# kubestronaut-certification-predictor
